{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowball stemming\n",
    "### Snowball stemming, also known as the Porter2 stemming algorithm, is another popular stemming algorithm used in natural language processing (NLP). \n",
    "### It is an improvement over the original Porter stemming algorithm, providing more accurate and linguistically robust stemming. \n",
    "### The Snowball stemming algorithm is language-specific and supports multiple languages. \n",
    "### It follows a similar approach to the Porter stemming algorithm, applying a set of rules to strip suffixes from words. However, Snowball stemming incorporates additional linguistic knowledge and fine-tunes the stemming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cared ----> care\n",
      "university ----> univers\n",
      "fairly ----> fair\n",
      "easily ----> easili\n",
      "singing ----> sing\n",
      "sings ----> sing\n",
      "sung ----> sung\n",
      "singer ----> singer\n",
      "sportingly ----> sport\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "#the stemmer requires a language parameter\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    " \n",
    "#list of tokenized words\n",
    "words = ['cared','university','fairly','easily','singing',\n",
    "       'sings','sung','singer','sportingly']\n",
    " \n",
    "#stem's of each word\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    x = snow_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "     \n",
    "#print stemming results\n",
    "for e1,e2 in zip(words,stem_words):\n",
    "    print(e1+' ----> '+e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemming\n",
    "### Porter stemming is a widely used stemming algorithm in natural language processing (NLP). Stemming is the process of reducing words to their base or root form, which helps in grouping together different variations of the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'ran']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create an instance of the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"runs\", \"ran\"]\n",
    "\n",
    "# Stem each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancaster stemming\n",
    "### Lancaster stemming is another popular stemming algorithm used in natural language processing (NLP). Like Porter and Snowball stemming, Lancaster stemming aims to reduce words to their base or root form. However, it employs a more aggressive stemming approach, often resulting in shorter stems compared to other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'ran']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Create an instance of the LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"runs\", \"ran\"]\n",
    "\n",
    "# Stem each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams using nltk\n",
    "### In natural language processing (NLP), an n-gram is a contiguous sequence of n items from a given text, where the items can be words, characters, or even larger units such as phrases. The \"n\" in n-gram represents the number of items in the sequence.\n",
    "### Unigram: A unigram is an n-gram of size 1, where each item in the sequence is a single word. For example, in the sentence \"I love to code,\" the unigrams would be \"I,\" \"love,\" \"to,\" and \"code.\"\n",
    "### Bigram: A bigram is an n-gram of size 2, where each item in the sequence consists of two consecutive words. For example, in the same sentence \"I love to code,\" the bigrams would be \"I love,\" \"love to,\" and \"to code.\"\n",
    "### Trigram: A trigram is an n-gram of size 3, where each item in the sequence consists of three consecutive words. For example, in the sentence \"I love to code,\" the trigrams would be \"I love to\" and \"love to code.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence: Hello, how are you, what are you doing, they both get placed finallu\n",
      "Enter the value of n: 3\n",
      "('Hello,', 'how', 'are')\n",
      "('how', 'are', 'you,')\n",
      "('are', 'you,', 'what')\n",
      "('you,', 'what', 'are')\n",
      "('what', 'are', 'you')\n",
      "('are', 'you', 'doing,')\n",
      "('you', 'doing,', 'they')\n",
      "('doing,', 'they', 'both')\n",
      "('they', 'both', 'get')\n",
      "('both', 'get', 'placed')\n",
      "('get', 'placed', 'finallu')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = input(\"Enter the sentence: \")\n",
    "n = int(input(\"Enter the value of n: \"))\n",
    "n_grams = ngrams(sentence.split(), n)\n",
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams without using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is', 'an')\n",
      "('is', 'an', 'example')\n",
      "('an', 'example', 'sentence')\n",
      "('example', 'sentence', 'for')\n",
      "('sentence', 'for', 'n-gram')\n",
      "('for', 'n-gram', 'generation.')\n"
     ]
    }
   ],
   "source": [
    "# Input sentence\n",
    "sentence = \"This is an example sentence for n-gram generation.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = sentence.split()\n",
    "\n",
    "# Set the value of n for n-gram generation\n",
    "n = 3\n",
    "\n",
    "# Generate n-grams\n",
    "ngrams_list = []\n",
    "for i in range(len(tokens) - n + 1):\n",
    "    ngram = tokens[i:i+n]\n",
    "    ngrams_list.append(tuple(ngram))\n",
    "\n",
    "# Print the generated n-grams\n",
    "for ngram in ngrams_list:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram smooting\n",
    "### N-gram smoothing, also known as add-one smoothing or Laplace smoothing, is a technique used in language modeling to address the problem of zero probabilities for unseen n-grams. It helps to overcome the sparsity issue and assign non-zero probabilities to unseen or rare n-grams. The basic idea behind n-gram smoothing is to adjust the probability estimates for n-grams by adding a constant value (typically 1) to the count of each n-gram. This effectively redistributes the probability mass from seen n-grams to unseen n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory\n",
    "# Here's a step-by-step approach for implementing add-one smoothing for n-grams:\n",
    "# 1.) Count the occurrences of each n-gram in your training corpus. Let's assume you have a dictionary or data structure that stores the counts.\n",
    "# 2.) Calculate the total count of all n-grams in the training corpus. This is the sum of the counts of all n-grams.\n",
    "# 3.) Determine the vocabulary size, which represents the number of unique n-grams in the training corpus. This is the number of keys or distinct items in your n-gram count dictionary.\n",
    "# 4.) Calculate the smoothed probability for each n-gram using the formula:\n",
    "# P_smoothed = (count(n-gram) + 1) / (total_count + vocabulary_size)\n",
    "# In this formula, count(n-gram) is the count of the specific n-gram, total_count is the sum of all n-gram counts, and vocabulary_size is the number of unique n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is') 0.14285714285714285\n",
      "('is', 'an') 0.14285714285714285\n",
      "('an', 'example') 0.14285714285714285\n",
      "('example', 'sentence') 0.14285714285714285\n",
      "('sentence', 'for') 0.14285714285714285\n",
      "('for', 'n-gram') 0.14285714285714285\n",
      "('n-gram', 'smoothing.') 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Input corpus\n",
    "corpus = \"This is an example sentence for n-gram smoothing.\"\n",
    "\n",
    "# Tokenize the corpus into words\n",
    "tokens = corpus.split()\n",
    "\n",
    "# Set the value of n for n-gram generation\n",
    "n = 2\n",
    "\n",
    "# Initialize n-gram count dictionary\n",
    "ngram_counts = defaultdict(int)\n",
    "\n",
    "# Generate n-grams and count their occurrences\n",
    "for i in range(len(tokens) - n + 1):\n",
    "    ngram = tuple(tokens[i:i+n])\n",
    "    ngram_counts[ngram] += 1\n",
    "\n",
    "# Calculate total count and vocabulary size\n",
    "total_count = sum(ngram_counts.values())\n",
    "vocabulary_size = len(ngram_counts)\n",
    "\n",
    "# Apply add-one smoothing and calculate smoothed probabilities\n",
    "smoothed_probabilities = {}\n",
    "for ngram, count in ngram_counts.items():\n",
    "    smoothed_probabilities[ngram] = (count + 1) / (total_count + vocabulary_size)\n",
    "\n",
    "# Print the smoothed probabilities\n",
    "for ngram, probability in smoothed_probabilities.items():\n",
    "    print(ngram, probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger\n",
    "### POS tagging, short for Part-of-Speech tagging, is the process of assigning a grammatical category (part-of-speech tag) to each word in a given text or sentence. The part-of-speech tags represent the syntactic or grammatical role of the word within the sentence, such as noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.\n",
    "### POS tagging is an important task in natural language processing (NLP) and is used in various applications, including text analysis, information extraction, machine translation, sentiment analysis, and more. It helps in understanding the structure of the sentence, disambiguating word meanings, and facilitating subsequent language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DT\n",
      "dog NN\n",
      "is VBZ\n",
      "chasing NN\n",
      "the DT\n",
      "ball NN\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Training data for the POS tagger\n",
    "training_data = [\n",
    "    (\"The cat is sitting on the mat\", \"DT NN VBZ VBG IN DT NN\"),\n",
    "    (\"I love to eat pizza\", \"PRP VBP TO VB NN\"),\n",
    "    (\"She is singing a song\", \"PRP VBZ VBG DT NN\")\n",
    "]\n",
    "\n",
    "# Prepare the training data in the required format\n",
    "tagged_sentences = [nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence, _ in training_data]\n",
    "\n",
    "# Train the POS tagger\n",
    "pos_tagger = nltk.DefaultTagger('NN')\n",
    "pos_tagger = nltk.UnigramTagger(tagged_sentences, backoff=pos_tagger)\n",
    "pos_tagger = nltk.BigramTagger(tagged_sentences, backoff=pos_tagger)\n",
    "\n",
    "# Test the POS tagger on new sentences\n",
    "test_sentence = \"The dog is chasing the ball\"\n",
    "tagged_words = nltk.word_tokenize(test_sentence)\n",
    "pos_tags = pos_tagger.tag(tagged_words)\n",
    "\n",
    "# Print the POS tags\n",
    "for word, tag in pos_tags:\n",
    "    print(word, tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunker\n",
    "### In natural language processing (NLP), a chunker, also known as a shallow parser or phrase chunker, is a component that groups words in a sentence into meaningful chunks based on their grammatical structure. Chunking is a process of identifying and labeling contiguous sequences of words that belong together syntactically, such as noun phrases, verb phrases, prepositional phrases, and more.\n",
    "### This can be helpful in various NLP tasks such as information extraction, named entity recognition, relation extraction, and syntactic parsing.They use the POS tags assigned to each word in a sentence to identify and group words into chunks based on predefined grammatical patterns or rules. These rules are often defined using regular expressions or other pattern matching techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT black/JJ cat/NN)\n",
      "  is/VBZ\n",
      "  sitting/VBG\n",
      "  (PP on/IN (NP the/DT mat/NN)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The black cat is sitting on the mat\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Define a chunk grammar using regular expressions\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}   # Chunk noun phrases\n",
    "    VP: {<VB.*><NP|PP>}    # Chunk verb phrases\n",
    "    PP: {<IN><NP>}         # Chunk prepositional phrases\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser using the defined grammar\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply the chunk parser to the part-of-speech tagged sentence\n",
    "chunks = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print the resulting chunks\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
